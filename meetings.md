---
title: Meetings
permalink: /meetings/
---

### <span style="color:#2348a3">MaVi meetings</span>


      <h2>Upcoming Seminars</h2>
      <table>
          <tr><td  style="vertical-align:top"><b>Date and Time (UK Time)</b></td> <td  style="vertical-align:top"><b>Speaker</b></td> <td  style="vertical-align:top"><b>Title</b></td> <td width="40%"><b>Abstract</b></td> <td  style="vertical-align:top"><b>ZoomLink</b></td> </tr>


          <tr><td  style="vertical-align:top">19 Oct 2021 4pm</td> <td  style="vertical-align:top"><a href="https://cihangxie.github.io/">Cihang Xie</a></td> <td  style="vertical-align:top">TBD</td> <td  style="vertical-align:top">TBD.</td> <td  style="vertical-align:top"><a href="https://bristol-ac-uk.zoom.us/j/92722444329">Join Zoom</a></td></tr>
          
      </table>
      
      <h2>Previous Seminars</h2>
      <table>
                  <tr><td  style="vertical-align:top">5 Oct 2021 2pm</td> <td  style="vertical-align:top"><a href="http://people.csail.mit.edu/roudi/">Andrew Rouditchenko</a></td> <td  style="vertical-align:top">TBD</td> <td  style="vertical-align:top">TBD.</td> <td  style="vertical-align:top"><a href="https://bristol-ac-uk.zoom.us/j/99598264206">Join Zoom</a></td></tr>
        <tr><td  style="vertical-align:top">21 Sep 2021 3pm</td> <td  style="vertical-align:top"><a href="https://fabiancaba.com/">Fabian Caba Heilbron</a></td> <td  style="vertical-align:top">Multimodal Learning for Creative Video Applications</td> <td  style="vertical-align:top">Watching and creating videos is a multimodal experience. To understand video, one needs to reason about the movements on screen, the meaning of a speech, and the sound of objects. In this talk, Fabian will discuss two recent works addressing inherently multimodal video problems. First, he will present novel architectures for speaker identification in videos that leverage Spatio-temporal audiovisual context. Then, he will share his quest and latest works on learning the anatomy of video editing. To conclude, he will open up the discussion with exciting frontiers in this rapidly evolving research space.</td> <td  style="vertical-align:top"><a href="https://bristol-ac-uk.zoom.us/j/91790767337">Join Zoom</a></td></tr>
          <tr><td  style="vertical-align:top"><b>Date and Time (UK Time)</b></td> <td  style="vertical-align:top"><b>Speaker</b></td> <td  style="vertical-align:top"><b>Title</b></td> <td width="40%"><b>Abstract</b></td> <td  style="vertical-align:top"><b>ZoomLink</b></td> </tr>
          <tr><td  style="vertical-align:top">7 Sep 2021</td> <td  style="vertical-align:top"><a href="http://users.cecs.anu.edu.au/~cherian/">Anoop Cherian</a></td> <td  style="vertical-align:top">InSeGAN: An Unsupervised Approach to Identical Instance Segmentation</td> <td  style="vertical-align:top">Identifying (nearly) identical instances of objects is a problem that is ubiquitous in daily life. For example, when taking a paperclip from a container, choosing an apple from a box, or removing a book from a library shelf, humans subconsciously solve this problem because we have an understanding of what the individual instances are. However, when robots are deployed for such a picking task, they need to be able to identify the instances for planning their grasp and approach. In many of these scenarios, the robot's owners often have no access to a 3D model of the object to be picked, and annotating individual instances for training can be costly, inconvenient, and unscalable. However, they may have access to collections of unlabeled images each containing multiple instances of the object, such as for example, just shaking the paperclip container. In this talk, I will introduce InSeGAN -- an unsupervised algorithm based on a 3D generative adversarial network (GAN) for segmenting instances of identical rigid objects in depth images. InSeGAN is an analysis-by-synthesis approach in which we design a novel GAN architecture to synthesize multiple-instance depth images with independent control over each instance, and which at test time can be reconfigured to produce instance segmentations for real-world depth images. To empirically validate the benefits of InSeGAN, I will showcase experiments on several synthetic and robot-collected-real-world datasets; our results outperforming several classical and very recent deep learning methods by large margins.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
          
          <tr><td  style="vertical-align:top">8 June 2021</td> <td  style="vertical-align:top"><a href="http://zhegan27.github.io/">Zhe Gan</a></td> <td  style="vertical-align:top">Recent Advances in Vision-Language Pre-training</td> <td  style="vertical-align:top">With the advent of models such as OpenAI CLIP and DALL-E, transformer-based vision-language pre-training has become an increasingly hot research topic. In this talk, I will share some of our recent work in this field that is published in NeurIPS 2020, ECCV 2020 and EMNLP 2020. Specifically, I will answer the following questions. First, how to perform vision-language pre-training? Second, how to understand what has been learned in the pre-trained models? Third, how to enhance the performance of pre-trained models via adversarial training? And finally, how can we extend image-text pre-training to video-text pre-training? Accordingly, I will present UNITER, VALUE, VILLA and HERO to answer these four questions. At last, I will also briefly discuss the challenges and future directions for vision-language pre-training.</td> <td  style="vertical-align:top">EXPIRED</td></tr>
          
          <tr><td  style="vertical-align:top">18 May 2021</td> <td  style="vertical-align:top"><a href="https://i.cs.hku.hk/~kykwong/">Kenneth Wong</a></td> <td  style="vertical-align:top"> Deep Photometric  Stereo for Non-Lambertian Surfaces</td> <td  style="vertical-align:top">In this talk, we will introduce our recently proposed deep neural networks for solving the photometric stereo problem for non-Lambertian surfaces. Traditional approaches often adopt simplified reflectance models and constrained setups to make the problem more tractable, but this greatly hinders their applications on real-world objects. We propose a deep fully convolutional network, named PS-FCN, that predicts a normal map of an  object from an arbitrary number of images captured under different light directions. Compared with other learning based methods, PS-FCN does not depend on a pre-defined set of light directions and can handle multiple images in an order-agnostic manner. To tackle uncalibrated light directions, we propose a deep convolutional neural network, named LCNet, that predicts per-image light direction and intensity from both local and global features extracted from all input images. We analyze the features learned by LCNet and find they resemble attached shadows, shadings, and specular highlights, which are known to provide clues in resolving  GBR ambiguity. Based on this insight, we propose a guided calibration network, named GCNet, that explicitly leverages object shape and shading information for improved lighting estimation. Experiments on synthetic and real data will be presented to demonstrate the effectiveness of our proposed networks.</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">4 May 2021</td> <td  style="vertical-align:top"><a href="https://www.cs.cmu.edu/~aayushb/">Aayush Bansal</a></td> <td  style="vertical-align:top">Unsupervised Exemplar Representations: Beyond Task-based Optimization</td> <td  style="vertical-align:top">We are tuned to think in terms of "tasks" in computer vision, graphics, machine learning, and robotics. Given a problem, we collect a large amount of domain-specific and task-specific data. We then get appropriate human annotations on massive datasets to enable highly-tuned optimizations based on supervised learning. I believe this setup is prohibitive and has hindered progress in the community. In this talk, I will present two unsupervised approaches that build on a simple autoencoder. In the first part of the talk, I will talk about Exemplar Autoencoders trained on an individual's voice and yet generalizes for unknown voices in different languages. I will demonstrate its application as an assistive tool for speech-impaired and multi-lingual translation. In the second part of the talk, I will talk about Video-Specific Autoencoders trained on random frames of a video without temporal information via a simple reconstruction loss. The learned representation allows us to do a wide variety of video analytic tasks such as (but not limited to) spatial and temporal super-resolution, spatial and temporal editing, video textures, average video exploration, and correspondence estimation within and across videos. Neither approach optimizes for the end task and still competes with state-of-the-art supervised methods that do task-specific optimization.</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">20 April 2021</td> <td  style="vertical-align:top"><a href="http://www.columbia.edu/~zs2262/">Mike Shou</a></td> <td  style="vertical-align:top">Generic Event Boundary Detection: A Benchmark for Event Segmentation</td> <td  style="vertical-align:top">n this talk, I will present a novel task together with a new benchmark for detecting generic, taxonomy-free event boundaries that segment a whole video into chunks. Conventional work in temporal video segmentation and action detection focuses on localizing pre-defined action categories and thus does not scale to generic videos. Cognitive Science has known since last century that humans consistently segment videos into meaningful temporal chunks. This segmentation happens naturally, without pre-defined event categories and without being explicitly asked to do so. Here, we repeat these cognitive experiments on mainstream CV datasets; with our novel annotation guideline which addresses the complexities of taxonomy-free event boundary annotation, we introduce the task of Generic Event Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our Kinetics-GEBD has the largest number of boundaries (e.g. 32x of ActivityNet, 8x of EPIC-Kitchens-100) which are in-the-wild, open-vocabulary, cover generic event change, and respect human perception diversity. We view GEBD as an important stepping stone towards understanding the video as a whole, and believe it has been previously neglected due to a lack of proper task definition and annotations. Through experiment and human study we demonstrate the value of the annotations. Further, we benchmark supervised and un-supervised GEBD approaches on the TAPOS dataset and our Kinetics-GEBD, together with method design explorations that suggest future directions. We release our annotations and baseline codes at CVPR'21 LOVEU Challenge: https://sites.google.com/view/loveucvpr21</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">23 Mar 2021</td> <td  style="vertical-align:top"><a href="http://renaud-detry.net">Renaud Detry</a></td> <td  style="vertical-align:top">Autonomous robot manipulation for planetary and terrestrial applications.</td> <td  style="vertical-align:top">In this talk, I will first discuss the experimental validation of autonomous robot behaviors that support the exploration of Mars' surface, lava tubes on Mars and the Moon, icy bodies and ocean worlds, and operations on orbit around the Earth. I will frame the presentation with the following questions: What new insights or limitations arise when applying algorithms to real-world data as opposed to benchmark datasets or simulations? How can we address the limitations of real-world environments—e.g., noisy or sparse data, non-i.i.d. sampling, etc.? What challenges exist at the frontiers of robotic exploration of unstructured and extreme environments? I will discuss our approach to validating autonomous machine-vision capabilities for the notional Mars Sample Return campaign, for autonomously navigating lava tubes, and for autonomously assembling modular structures on orbit. The talk will highlight the thought process that drove the decomposition of a validation need into a collection of tests conducted on off-the-shelf datasets, custom/application-specific datasets, and simulated or physical robot hardware, where each test addressed a different range of experimental parameters for sensing/actuation fidelity, breadth of environmental conditions, and breadth of jointly-tested robot functions. Next, I will present a task-oriented grasp model, that en- codes grasps that are configurationally compatible with a given task. The model consists of two independent agents: First, a geometric grasp model that computes, from a depth image, a distribution of 6D grasp poses for which the shape of the gripper matches the shape of the underlying surface. The model relies on a dictionary of geometric object parts annotated with workable gripper poses and preshape parameters. It is learned from experience via kinesthetic teaching. The second agent is a CNN-based semantic model that identifies grasp- suitable regions in a depth image, i.e., regions where a grasp will not impede the execution of the task. The semantic model allows us to encode relationships such as “grasp from the handle.” A key element of this work is to use a deep network to integrate contextual task cues, and defer the structured-output problem of gripper pose computation to an explicit (learned) geometric model. Jointly, these two models generate grasps that are mechanically fit, and that grip on the object in a way that enables the intended task.</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">16 Feb 2021</td> <td  style="vertical-align:top"><a href="https://www.dimtzionas.com/">Dimitris Tzionas</a></td> <td  style="vertical-align:top">From Interacting Hands to Expressive and Interacting Humans</td> <td  style="vertical-align:top">A long-term goal of computer vision and artificial intelligence is to develop human-centred AI that perceives humans in their environments and helps them accomplish their tasks. For this, we need holistic 3D scene understanding, namely modelling how people and objects look, estimating their 3D shape and pose, and inferring their semantics and spatial relationships. For humans and animals this perceptual capability seems effortless, however, endowing computers with similar capabilities has proven to be hard. Fundamentally, the problem involves observing a scene through cameras, and inferring the configuration of humans and objects from images. Challenges exist at all levels of abstraction, from the ill-posed 3D inference from noisy 2D images, to the semantic interpretation of it. The talk will discuss several projects (IJCV’16, TOG’17, CVPR’19, ICCV’19, ECCV’20) that attempt to understand, formalize, and model increasingly complex cases of human-object interactions. These cases range from interacting hands to expressive and interacting whole-body humans. More specifically, the talk will present novel statistical models of the human hand and the whole body, and the usage of these models (1) to efficiently regularize 3D reconstruction from monocular 2D images and eventually (2) to build statistical models of interactions. The presented models are freely available for research purposes.</td> <td  style="vertical-align:top">EXPIRED</tr>
          
          <tr><td  style="vertical-align:top">2 Feb 2021 3pm</td> <td  style="vertical-align:top"><a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a></td> <td  style="vertical-align:top">3D Photography and Videography</td> <td  style="vertical-align:top"> Images and videos allow us to capture and share memorable moments of our lives. However, 2D images and videos appear flat due to the lack of depth perception. In this talk, I will present our recent efforts to overcome these limitations. Specifically, I will cover our recent work for creating compelling 3D photography, estimating consistent video depth for advanced video-based visual effects, and free-viewpoint videos. I will conclude the talk with some ongoing research and research challenges ahead. </td> <td  style="vertical-align:top">EXPIRED</td></tr>
          
          
      </table>

<br>

#### Other relevant meetings:

<!---
#<b>[Neural dynamics forum](https://ndforum.blogs.bristol.ac.uk/)</b> <br>
#Combined experimental and computational seminars organised by students of the Neural Dynamics Wellcome Trust doctoral program.

#Join our mailing list:
#<form method="POST" action="https://formspree.io/conor.houghton@bristol.ac.uk">
#  <input type="email" name="email" placeholder="Your email">&nbsp;&nbsp;&nbsp;
#  <textarea name="message" style="display:none;" placeholder="Please add me to the CNU mailing list.">Please add me to the CNU mailing list.</textarea>
#  <button type="submit">Send</button>
#</form>
-->

<hr>
{% include footer.html %}